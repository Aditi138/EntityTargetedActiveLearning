Namespace(SPAN_wise=False, addbias=False, aug_lang='english', aug_lang_train_path='/home/aschaudh/EntityTargetedActiveLearning/data/Spanish/vocab.conll', batch_size=10, brown_cluster_dim=30, brown_cluster_num=0, brown_cluster_path=None, cap=False, char_emb_dim=30, char_hidden_dim=25, cnn_filter_size=30, cnn_win_size=3, data_aug=False, debug=None, decay_rate=0.05, dev_filename_path=None, dev_path='/home/aschaudh/EntityTargetedActiveLearning/data/Spanish/esp.dev', dynet_gpu=None, dynet_mem=1000, dynet_seed=3278657, emb_dropout_rate=0.3, entropy_threshold=None, eval_folder='../eval', feature_birnn_hidden_dim=50, feature_dim=10, fineTune=False, fixedVocab=True, full_data_path='/home/aschaudh/EntityTargetedActiveLearning/data/Spanish/transferred_data.conll', hidden_dim=200, init_lr=0.015, k=200, lang='es', layer=1, load_from_path=None, lr_decay=False, map_dim=100, map_pretrain=False, misc=False, mode='train', model_arc='char_cnn', model_name='test', monolingual_data_path=None, new_test_conll='../datasets/english/eng.test.bio.conll', new_test_path='../datasets/english/eng.test.bio.conll', ngram=2, output_dropout_rate=0.5, patience=3, pos_emb_dim=50, pretrain_emb_path='/home/aschaudh/EntityTargetedActiveLearning/data/Spanish/esp.vec', pretrain_fix=False, remove_singleton=False, replace_unk_rate=0.0, res_discrete_feature=False, rnn_type='lstm', run=0, save_to_path='../saved_models/test.model', split_hashtag=False, tag_emb_dim=50, tagging_scheme='bio', test_conll=True, test_filename_path=None, test_path='/home/aschaudh/EntityTargetedActiveLearning/data/Spanish/esp.test', tgt_lang_train_path='../datasets/english/eng.train.bio.conll', to_annotate='./annotate.txt', tot_epochs=1000, train_ensemble=False, train_filename_path=None, train_path='/home/aschaudh/EntityTargetedActiveLearning/data/Spanish/transferred_data.conll', use_CFB=False, use_brown_cluster=False, use_discrete_features=False, use_partial=False, valid_freq=1300, word_emb_dim=100)
[dynet] random seed: 3278657
[dynet] allocating memory: 512MB
[dynet] memory allocation done.
PREFIX: test_2e21c0
Generating vocabs from training file ....
Singleton words number: 12608
Size of vocab before: 26099
Size of vocab after: 26101
Loading pretrained embeddings from /home/aschaudh/EntityTargetedActiveLearning/data/Spanish/esp.vec.
length of dict: 26101
(26101, 26101)
Word number not covered in pretrain embedding: 1869
Size of vocab after: 408914
NER tag num=9, Word vocab size=408914, Char Vocab size=93
{0: 'B-LOC', 1: 'B-PER', 2: 'B-GPE', 3: 'B-ORG', 4: 'I-LOC', 5: 'I-PER', 6: 'I-GPE', 7: 'I-ORG', 8: 'O'}
Processed 1000 training data.
Processed 2000 training data.
Processed 3000 training data.
Processed 4000 training data.
Processed 5000 training data.
Processed 6000 training data.
Processed 7000 training data.
Processed 8000 training data.
Processed 9000 training data.
Processed 10000 training data.
Processed 11000 training data.
Processed 12000 training data.
Processed 13000 training data.
Data set size (train): 13096
('Number of discrete features: ', 0)
Using Char CNN model!
In CNN encoder: creating lookup embedding!
In NER CRF: Using pretrained word embedding!
Epoch = 0, Updates = 100, CRF Loss=5.156352, Accumulative Loss=8.562427.
Epoch = 0, Updates = 200, CRF Loss=12.386701, Accumulative Loss=6.830945.
Epoch = 0, Updates = 300, CRF Loss=0.718073, Accumulative Loss=5.881439.
Epoch = 0, Updates = 400, CRF Loss=1.274954, Accumulative Loss=5.156480.
Epoch = 0, Updates = 500, CRF Loss=1.253865, Accumulative Loss=4.620079.
